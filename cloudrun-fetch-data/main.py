#!/usr/bin/env python3
"""
Cloud Run: BTCå†å²æ•°æ®è·å–æœåŠ¡
- æ”¯æŒå¤šæ—¶é—´å‘¨æœŸï¼ˆ1mã€5mã€15mã€1hã€4hã€1dï¼‰
- è‡ªåŠ¨ä¿å­˜åˆ°GCS
- å¤„ç†å¤§æ•°æ®é‡ï¼ˆ18ä¸ªæœˆæ•°æ®ï¼‰
- äº‘ç¯å¢ƒç›´è¿ï¼Œæ— éœ€ä»£ç†
- é€‚åˆé•¿æ—¶é—´è¿è¡Œä»»åŠ¡
"""

import os
import json
import time
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
import logging

# å»¶è¿Ÿå¯¼å…¥é‡é‡çº§åº“ï¼Œé¿å…å¯åŠ¨æ—¶åŠ è½½å¤±è´¥
def get_ccxt():
    import ccxt
    return ccxt

def get_pandas():
    import pandas as pd
    return pd

def get_storage():
    from google.cloud import storage
    return storage

app = Flask(__name__)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def fetch_btc_data_cloud(timeframe='5m', days=540, save_gcs=True):
    """
    åœ¨äº‘ç¯å¢ƒä¸­è·å–BTCæ•°æ®
    Args:
        timeframe: æ—¶é—´å‘¨æœŸ (1m, 5m, 15m, 1h, 4h, 1d)
        days: è·å–å¤©æ•° (540å¤© â‰ˆ 18ä¸ªæœˆ)
        save_gcs: æ˜¯å¦ä¿å­˜åˆ°GCS
    """
    # æ—¶é—´å‘¨æœŸé…ç½®
    timeframe_config = {
        '1m': {'freq': '1min', 'interval_ms': 60000, 'display': '1åˆ†é’Ÿ'},
        '5m': {'freq': '5min', 'interval_ms': 300000, 'display': '5åˆ†é’Ÿ'},
        '15m': {'freq': '15min', 'interval_ms': 900000, 'display': '15åˆ†é’Ÿ'},
        '1h': {'freq': '1H', 'interval_ms': 3600000, 'display': '1å°æ—¶'},
        '4h': {'freq': '4H', 'interval_ms': 14400000, 'display': '4å°æ—¶'},
        '1d': {'freq': '1D', 'interval_ms': 86400000, 'display': '1å¤©'}
    }
    
    if timeframe not in timeframe_config:
        raise ValueError(f"ä¸æ”¯æŒçš„æ—¶é—´å‘¨æœŸ: {timeframe}ï¼Œæ”¯æŒ: {list(timeframe_config.keys())}")
    
    config = timeframe_config[timeframe]
    logger.info(f"ğŸ”„ Cloud Runè·å–BTCUSDTæœ€è¿‘{days}å¤©çš„{config['display']}æ•°æ®...")
    
    # äº‘ç¯å¢ƒäº¤æ˜“æ‰€é…ç½®ï¼ˆç›´è¿ï¼Œä¸éœ€è¦ä»£ç†ï¼‰
    exchange_config = {
        'rateLimit': 800,   # Cloud Runå¯ä»¥æ›´å¿«ä¸€äº›
        'enableRateLimit': True,
        'timeout': 30000,   # 30ç§’è¶…æ—¶
        'sandbox': False,
    }
    
    logger.info("ğŸŒ Cloud Runäº‘ç¯å¢ƒç›´è¿æ¨¡å¼")
    
    try:
        ccxt = get_ccxt()
        exchange = ccxt.binance(exchange_config)
        logger.info("ğŸ” æµ‹è¯•äº‘ç¯å¢ƒäº¤æ˜“æ‰€è¿æ¥...")
        server_time = exchange.fetch_time()
        if server_time:
            logger.info(f"âœ… Cloud Runè¿æ¥æˆåŠŸï¼ŒæœåŠ¡å™¨æ—¶é—´: {datetime.fromtimestamp(server_time/1000)}")
    except Exception as e:
        logger.error(f"âŒ Cloud Runè¿æ¥å¤±è´¥: {e}")
        raise
    
    # è®¡ç®—æ•°æ®é‡
    interval_minutes = config['interval_ms'] // 60000
    total_expected_records = (days * 24 * 60) // interval_minutes
    logger.info(f"ğŸ“Š é¢„æœŸè·å–çº¦ {total_expected_records:,} æ¡æ•°æ®")
    
    # è®¡ç®—æ—¶é—´èŒƒå›´
    end_time = datetime.now()
    start_time = end_time - timedelta(days=days)
    
    logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_time.strftime('%Y-%m-%d %H:%M')} åˆ° {end_time.strftime('%Y-%m-%d %H:%M')}")
    
    # è·å–æ•°æ®
    symbol = 'BTC/USDT'
    since = int(start_time.timestamp() * 1000)
    end_timestamp = int(end_time.timestamp() * 1000)
    
    all_ohlcv = []
    current_since = since
    batch_count = 0
    start_time_fetch = time.time()
    
    logger.info("ğŸ“¡ Cloud Runå¼€å§‹æ‰¹é‡è·å–æ•°æ®...")
    
    # Cloud Runä¼˜åŒ–çš„æ‰¹å¤„ç†å‚æ•°
    max_retries = 5
    base_delay = 0.3  # Cloud Runå¯ä»¥æ›´å¿«
    
    while current_since < end_timestamp:
        retry_count = 0
        batch_success = False
        
        while retry_count < max_retries and not batch_success:
            try:
                logger.info(f"ğŸ”„ æ‰¹æ¬¡ {batch_count + 1} (æ€»è¿›åº¦: {len(all_ohlcv):,})")
                ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since=current_since, limit=1000)
                
                if not ohlcv:
                    logger.info("âš ï¸ æ²¡æœ‰æ›´å¤šæ•°æ®")
                    batch_success = True
                    break
                    
                all_ohlcv.extend(ohlcv)
                batch_count += 1
                batch_success = True
                
                # è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤º
                current_records = len(all_ohlcv)
                progress_percent = min((current_records / total_expected_records) * 100, 100)
                elapsed_time = time.time() - start_time_fetch
                
                if elapsed_time > 0:
                    records_per_second = current_records / elapsed_time
                    if records_per_second > 0:
                        eta_seconds = max(0, (total_expected_records - current_records) / records_per_second)
                        eta_str = f", ETA: {int(eta_seconds//60)}åˆ†{int(eta_seconds%60)}ç§’"
                    else:
                        eta_str = ""
                else:
                    eta_str = ""
                
                current_since = ohlcv[-1][0] + config['interval_ms']
                
                logger.info(f"ğŸ“Š æ‰¹æ¬¡ {batch_count} | {progress_percent:.1f}% ({current_records:,}/{total_expected_records:,}){eta_str}")
                
                # Cloud Runå»¶è¿Ÿä¼˜åŒ–
                time.sleep(base_delay)
                
            except Exception as e:
                retry_count += 1
                if retry_count < max_retries:
                    logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_count + 1} é‡è¯• {retry_count}/{max_retries}: {e}")
                    time.sleep(base_delay * retry_count * 2)  # é€’å¢å»¶è¿Ÿ
                else:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_count + 1} å¤±è´¥: {e}")
                    # è·³è¿‡è¿™æ‰¹æ•°æ®ï¼Œç»§ç»­ä¸‹ä¸€æ‰¹
                    current_since += config['interval_ms'] * 1000
                    batch_success = True
    
    if not all_ohlcv:
        raise ValueError("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•æ•°æ®")
    
    total_fetch_time = time.time() - start_time_fetch
    logger.info(f"âœ… è·å–å®Œæˆ! {len(all_ohlcv)} æ¡æ•°æ®ï¼Œè€—æ—¶ {total_fetch_time:.1f}ç§’")
    
    # æ•°æ®å¤„ç†ï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
    logger.info("ğŸ”§ å¤„ç†æ•°æ®...")
    pd = get_pandas()
    df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
    df = df.set_index('datetime')
    
    # ç®€åŒ–çš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ï¼ˆé€‚åˆäº‘ç¯å¢ƒï¼‰
    logger.info("ğŸ“Š è®¡ç®—åŸºç¡€æŠ€æœ¯æŒ‡æ ‡...")
    
    if timeframe == '5m':
        short_ma, long_ma = 4, 12
    elif timeframe == '1h':
        short_ma, long_ma = 20, 60
    else:
        short_ma, long_ma = 10, 30
    
    df['returns'] = df['close'].pct_change()
    df['ma_short'] = df['close'].rolling(short_ma, min_periods=1).mean()
    df['ma_long'] = df['close'].rolling(long_ma, min_periods=1).mean()
    df['ma_ratio'] = df['close'] / df['ma_short']
    df['volatility'] = df['returns'].rolling(short_ma, min_periods=1).std()
    
    # æ¸…ç†æ•°æ®
    df_clean = df.fillna(0)
    
    logger.info(f"ğŸ“ˆ å¤„ç†å®Œæˆ: {df_clean.shape} | æ—¶é—´èŒƒå›´: {df_clean.index.min()} åˆ° {df_clean.index.max()}")
    
    # ä¿å­˜åˆ°GCS
    gcs_path = None
    if save_gcs:
        try:
            bucket_name = os.getenv('BUCKET_NAME', 'ai4fnew-drl-btc-20250827')
            storage = get_storage()
            client = storage.Client()
            bucket = client.bucket(bucket_name)
            
            gcs_filename = f"btc_data_{timeframe}_{days}d.csv"
            gcs_path = f"data/{gcs_filename}"
            blob = bucket.blob(gcs_path)
            
            # ç›´æ¥ä¸Šä¼ CSVæ•°æ®
            csv_data = df_clean.to_csv()
            blob.upload_from_string(csv_data, content_type='text/csv')
            
            logger.info(f"â˜ï¸ æ•°æ®å·²ä¿å­˜åˆ° GCS: gs://{bucket_name}/{gcs_path}")
            
        except Exception as e:
            logger.error(f"âš ï¸ ä¿å­˜åˆ°GCSæ—¶å‡ºé”™: {e}")
    
    return {
        'success': True,
        'records_count': len(df_clean),
        'data_shape': list(df_clean.shape),
        'time_range': {
            'start': str(df_clean.index.min()),
            'end': str(df_clean.index.max())
        },
        'columns': list(df_clean.columns),
        'gcs_path': f"gs://{bucket_name}/{gcs_path}" if gcs_path else None,
        'processing_time_seconds': total_fetch_time
    }

@app.route('/', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'service': 'btc-data-fetcher',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/fetch', methods=['POST', 'GET'])
def fetch_data():
    """è·å–BTCæ•°æ®çš„ä¸»æ¥å£"""
    try:
        # è§£æå‚æ•°
        if request.method == 'POST':
            data = request.get_json() or {}
        else:
            data = request.args.to_dict()
        
        timeframe = data.get('timeframe', '5m')
        days = int(data.get('days', 540))  # é»˜è®¤18ä¸ªæœˆ
        save_gcs_value = data.get('save_gcs', 'true')
        if isinstance(save_gcs_value, bool):
            save_gcs = save_gcs_value
        else:
            save_gcs = str(save_gcs_value).lower() == 'true'
        
        logger.info(f"ğŸš€ å¼€å§‹è·å–æ•°æ®: timeframe={timeframe}, days={days}, save_gcs={save_gcs}")
        
        # æ‰§è¡Œæ•°æ®è·å–
        result = fetch_btc_data_cloud(timeframe=timeframe, days=days, save_gcs=save_gcs)
        
        logger.info(f"âœ… æ•°æ®è·å–å®Œæˆ: {result['records_count']} æ¡è®°å½•")
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®è·å–å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'æ•°æ®è·å–å¤±è´¥'
        }), 500

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(debug=False, host='0.0.0.0', port=port)