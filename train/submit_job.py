import argparse
import os
from datetime import datetime
from typing import Optional

from google.cloud import aiplatform, storage


def create_gcs_training_data(
    project_id: str, bucket_name: str, local_csv_path: Optional[str] = None
):
    """åˆ›å»ºæˆ–ä¸Šä¼ è®­ç»ƒæ•°æ®åˆ°GCS"""
    print(f"ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")

    try:
        client = storage.Client(project=project_id)
        bucket = client.bucket(bucket_name)

        # å¦‚æœä¼ å…¥çš„æ˜¯GCSè·¯å¾„ï¼Œç›´æ¥éªŒè¯å¹¶ä½¿ç”¨
        if local_csv_path and local_csv_path.startswith("gs://"):
            # éªŒè¯GCSæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            try:
                # è§£æGCSè·¯å¾„ gs://bucket/path
                path_parts = local_csv_path.replace("gs://", "").split("/", 1)
                bucket_name_from_path = path_parts[0]
                blob_path = path_parts[1] if len(path_parts) > 1 else ""
                
                check_bucket = client.bucket(bucket_name_from_path)
                check_blob = check_bucket.blob(blob_path)
                
                if check_blob.exists():
                    print(f"âœ… ä½¿ç”¨ç°æœ‰GCSæ•°æ®: {local_csv_path}")
                    return local_csv_path
                else:
                    raise FileNotFoundError(f"GCSæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {local_csv_path}")
            except FileNotFoundError:
                raise
            except Exception as e:
                print(f"âŒ éªŒè¯GCSæ•°æ®å¤±è´¥: {e}")
                raise
        
        # å¦‚æœæœ‰æœ¬åœ°CSVæ–‡ä»¶ï¼Œä¸Šä¼ åˆ°GCS
        elif local_csv_path and os.path.exists(local_csv_path):
            blob_name = "data/btc_1m.csv"
            blob = bucket.blob(blob_name)

            print(
                f"ğŸ“¤ ä¸Šä¼ æœ¬åœ°æ•°æ®: {local_csv_path} -> gs://{bucket_name}/{blob_name}"
            )
            blob.upload_from_filename(local_csv_path)
            print(f"âœ… æ•°æ®ä¸Šä¼ å®Œæˆ")

            return f"gs://{bucket_name}/{blob_name}"
        else:
            # è¿™é‡Œä¸è¦å†"éå† GCS é»˜è®¤è·¯å¾„å°è¯•"ï¼Œç›´æ¥ç¡¬å¤±è´¥
            raise FileNotFoundError("æœªæä¾›æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆæœ¬åœ°æˆ– GCSï¼‰ï¼Œæ— æ³•æäº¤è®­ç»ƒã€‚")

    except Exception as e:
        print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        raise


def submit_vertex_training_job(
    project_id: str,
    region: str,
    bucket_name: str,
    job_display_name: str,
    data_csv_path: Optional[str] = None,
    timesteps: int = 100000,
    machine_type: str = "n1-standard-4",
) -> str:
    """æäº¤Vertex AIè‡ªå®šä¹‰è®­ç»ƒä»»åŠ¡"""

    # æäº¤å‰å†æ¬¡æ–­è¨€ï¼ˆåŒä¿é™©ï¼‰
    if not data_csv_path:
        raise ValueError("å¿…é¡»æä¾› --data_csv ä¸”è¯¥è·¯å¾„æœ‰æ•ˆï¼ˆæœ¬åœ°å°†è¢«ä¸Šä¼ æˆ– GCS å¿…é¡»å­˜åœ¨ï¼‰ã€‚")

    print(f"â˜ï¸ æäº¤Vertex AIè®­ç»ƒä»»åŠ¡...")
    print(f"   - é¡¹ç›®ID: {project_id}")
    print(f"   - åŒºåŸŸ: {region}")
    print(f"   - ä»»åŠ¡åç§°: {job_display_name}")
    print(f"   - æœºå™¨ç±»å‹: {machine_type}")

    # åˆå§‹åŒ–Vertex AI
    aiplatform.init(
        project=project_id,
        location=region,
        staging_bucket=f"gs://{bucket_name}/vertex_output",
    )

    try:
        # å®šä¹‰è®­ç»ƒå®¹å™¨å‚æ•°
        output_dir = f"gs://{bucket_name}/models/ppo"
        args = [
            "--out_dir",
            output_dir,
            "--timesteps",
            str(timesteps),
            "--lookback",
            "60",
            "--lr",
            "3e-4",
        ]

        # å¦‚æœæœ‰è®­ç»ƒæ•°æ®è·¯å¾„ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
        if data_csv_path:
            args.extend(["--data_csv", data_csv_path])

        print(f"ğŸ“‹ è®­ç»ƒå‚æ•°: {' '.join(args)}")

        # åˆ›å»ºè‡ªå®šä¹‰è®­ç»ƒä»»åŠ¡
        job = aiplatform.CustomTrainingJob(
            display_name=job_display_name,
            script_path="train.py",  # è®­ç»ƒè„šæœ¬
            container_uri=f"gcr.io/cloud-aiplatform/training/pytorch-gpu.1-13.py39:latest",
            requirements=[
                "stable-baselines3>=2.0.0",
                "gymnasium>=0.28.0",
                "numpy>=1.21.0",
                "pandas>=1.3.0",
                "google-cloud-storage>=2.10.0",
                "torch>=1.13.0",
                "tensorboard>=2.11.0",
            ],
            model_serving_container_image_uri=None,  # ä¸ç”¨äºæ¨ç†
            project=project_id,
            location=region,
        )

        print(f"âœ… è®­ç»ƒä»»åŠ¡é…ç½®å®Œæˆ")

        # æäº¤è®­ç»ƒä»»åŠ¡
        print(f"ğŸš€ æäº¤è®­ç»ƒä»»åŠ¡...")

        model = job.run(
            args=args,
            replica_count=1,
            machine_type=machine_type,
            accelerator_type=None,  # CPUè®­ç»ƒ
            accelerator_count=0,
            base_output_dir=f"gs://{bucket_name}/vertex_output",
            sync=False,  # å¼‚æ­¥è¿è¡Œ
        )

        job_resource_name = job.resource_name
        print(f"ğŸ‰ è®­ç»ƒä»»åŠ¡å·²æäº¤æˆåŠŸï¼")
        print(f"   - ä»»åŠ¡èµ„æºåç§°: {job_resource_name}")
        print(
            f"   - åœ¨Google Cloud ConsoleæŸ¥çœ‹: https://console.cloud.google.com/vertex-ai/training/custom-jobs/{job_resource_name}?project={project_id}"
        )

        return job_resource_name

    except Exception as e:
        print(f"âŒ æäº¤è®­ç»ƒä»»åŠ¡å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        raise


def check_job_status(project_id: str, region: str, job_resource_name: str):
    """æ£€æŸ¥è®­ç»ƒä»»åŠ¡çŠ¶æ€"""
    print(f"ğŸ” æ£€æŸ¥è®­ç»ƒä»»åŠ¡çŠ¶æ€...")

    try:
        aiplatform.init(project=project_id, location=region)

        # ä»resource nameè·å–job
        job = aiplatform.CustomTrainingJob.get(job_resource_name)

        state = job.state
        print(f"   - å½“å‰çŠ¶æ€: {state}")

        if hasattr(job, "error") and job.error:
            print(f"   - é”™è¯¯ä¿¡æ¯: {job.error}")

        if hasattr(job, "create_time"):
            print(f"   - åˆ›å»ºæ—¶é—´: {job.create_time}")

        if hasattr(job, "update_time"):
            print(f"   - æ›´æ–°æ—¶é—´: {job.update_time}")

        return state

    except Exception as e:
        print(f"âŒ æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return None


def list_recent_jobs(project_id: str, region: str, limit: int = 5):
    """åˆ—å‡ºæœ€è¿‘çš„è®­ç»ƒä»»åŠ¡"""
    print(f"ğŸ“‹ æœ€è¿‘çš„è®­ç»ƒä»»åŠ¡ (æœ€å¤š {limit} ä¸ª):")

    try:
        aiplatform.init(project=project_id, location=region)

        # è·å–æœ€è¿‘çš„è®­ç»ƒä»»åŠ¡
        jobs = aiplatform.CustomTrainingJob.list(
            filter=None,
            order_by="create_time desc",
            project=project_id,
            location=region,
        )

        count = 0
        for job in jobs:
            if count >= limit:
                break

            print(f"   {count + 1}. {job.display_name}")
            print(f"      - çŠ¶æ€: {job.state}")
            print(f"      - èµ„æºåç§°: {job.resource_name}")
            if hasattr(job, "create_time"):
                print(f"      - åˆ›å»ºæ—¶é—´: {job.create_time}")
            print()

            count += 1

        if count == 0:
            print("   æš‚æ— è®­ç»ƒä»»åŠ¡")

    except Exception as e:
        print(f"âŒ è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æäº¤Vertex AI BTCè®­ç»ƒä»»åŠ¡")

    parser.add_argument("--project_id", type=str, required=True, help="GCPé¡¹ç›®ID")
    parser.add_argument(
        "--region", type=str, default="asia-southeast1", help="Vertex AIåŒºåŸŸ"
    )
    parser.add_argument("--bucket", type=str, required=True, help="GCSå­˜å‚¨æ¡¶åç§°")
    parser.add_argument("--data_csv", type=str, help="æœ¬åœ°è®­ç»ƒæ•°æ®CSVè·¯å¾„ (å¯é€‰)æˆ–GCSè·¯å¾„")
    parser.add_argument("--timesteps", type=int, default=100000, help="è®­ç»ƒæ­¥æ•°")
    parser.add_argument(
        "--machine_type", type=str, default="n1-standard-4", help="è®­ç»ƒæœºå™¨ç±»å‹"
    )
    parser.add_argument("--job_name", type=str, help="è‡ªå®šä¹‰ä»»åŠ¡åç§°")
    parser.add_argument("--list_jobs", action="store_true", help="åˆ—å‡ºæœ€è¿‘çš„è®­ç»ƒä»»åŠ¡")
    parser.add_argument(
        "--check_job", type=str, help="æ£€æŸ¥æŒ‡å®šä»»åŠ¡çŠ¶æ€ (æä¾›job resource name)"
    )

    args = parser.parse_args()

    print("=" * 60)
    print("â˜ï¸ Vertex AI BTC è®­ç»ƒä»»åŠ¡æäº¤å™¨")
    print("=" * 60)

    # åˆ—å‡ºæœ€è¿‘ä»»åŠ¡
    if args.list_jobs:
        list_recent_jobs(args.project_id, args.region, 10)
        return

    # æ£€æŸ¥ç‰¹å®šä»»åŠ¡çŠ¶æ€
    if args.check_job:
        check_job_status(args.project_id, args.region, args.check_job)
        return

    # ç”Ÿæˆä»»åŠ¡åç§°
    if not args.job_name:
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        job_name = f"drl-btc-training-{timestamp}"
    else:
        job_name = args.job_name

    try:
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        gcs_data_path = create_gcs_training_data(
            args.project_id, args.bucket, args.data_csv
        )

        # æäº¤è®­ç»ƒä»»åŠ¡
        job_resource_name = submit_vertex_training_job(
            project_id=args.project_id,
            region=args.region,
            bucket_name=args.bucket,
            job_display_name=job_name,
            data_csv_path=gcs_data_path,
            timesteps=args.timesteps,
            machine_type=args.machine_type,
        )

        print("=" * 60)
        print("âœ… è®­ç»ƒä»»åŠ¡å·²æˆåŠŸæäº¤ï¼")
        print(f"ğŸ“ ä»»åŠ¡åç§°: {job_name}")
        print(f"ğŸ”— èµ„æºåç§°: {job_resource_name}")
        print("ğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥çŠ¶æ€:")
        print(
            f"   python {__file__} --project_id {args.project_id} --region {args.region} --bucket {args.bucket} --check_job {job_resource_name}"
        )
        print("=" * 60)

    except Exception as e:
        print(f"âŒ ä»»åŠ¡æäº¤å¤±è´¥: {e}")
        exit(1)


if __name__ == "__main__":
    main()